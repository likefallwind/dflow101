# dflow101
With the advent of the AI4S era, concurrent learning, high-throughput computing, and distributed and heterogeneous infrastructure poke the pain points of the traditional workflow solutions. **dflow** aims to provide all scientists and engineers with scientific computing workflow. **dflow** is a cloud-native scientific computing workflow framework based on the Argo workflow engine that can be geared towards the needs and usage habits of the scientific computing community, combining the characteristics of AI, computing resources, and scientific computing.

Here, **dflow101** project hopes to encourage increasingly more scientific work using this kind of advanced workflow. 

## dflow documents
In [dflow repo](https://github.com/deepmodeling/dflow), there are several documents for your reference to help you get started quickly.
- [Overview](https://github.com/deepmodeling/dflow#1--overview)
- [Installation and Set-up](https://github.com/deepmodeling/dflow/tree/master/scripts)
- [User guide (dflow-docs)](https://github.com/deepmodeling/dflow#UserGuide)
- [Get-started tutorials](https://github.com/deepmodeling/dflow/tree/master/tutorials)

## dflow projects
- 

## Join us
We welcome anyone interested to join **dflow101**. The expected dflow project can be released in your own Github repo, including a README document, executable python scripts and supporting files (e.g. uploaded files, Dockerfile and others). README.md is expected to illustrate where this workflow is used and why using dflow is beneficial. 

And then you can submit your project in **Issues** and reviewers will **review** your projects and give their comments and suggestions in the form of **Issues**. Once your project is ready to be published, you are expected to add your dflow project's repo link in the second part here and pull request. 

Come on and join this fantastic project!


